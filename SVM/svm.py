# -*- coding: utf-8 -*-
"""SVM

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1790hsjfmH8f7Vg7x1liT2THKJsJVu6JW

# 2
"""

import pandas as pd
import numpy as np

train_data = np.loadtxt('train.csv', delimiter=',')
train_X = train_data[:, :-1]
train_y = train_data[:,4]

# Convert labels to {-1, 1}
train_y = (train_y == 1) * 2 - 1

# Load test data
test_data = np.loadtxt('test.csv', delimiter=',')
test_X = test_data[:, :-1]
test_y = test_data[:,4]

# Convert labels to {-1, 1}
test_y = (test_y == 1) * 2 - 1

# Scales Data.
def scale_data(X):
    mean = X.mean(axis=0)
    std = X.std(axis=0)
    X_scaled = (X - mean) / std
    return X_scaled

# Scale training data
train_X_scaled = scale_data(train_X)
# Scale test data
test_X_scaled = scale_data(test_X)


# Objective Function to diagnose conversion.
def objective_function(w, b, X, y, C):
    slack_variables = np.maximum(0, 1 - y * (np.dot(X, w) + b))
    return 0.5 * np.linalg.norm(w)**2 + C * np.sum(slack_variables)

# Gradient function.
def gradient(w, b, X, y, C):
    slack_variables = np.maximum(0, 1 - y * (np.dot(X, w) + b))

    # Calculate gradient for w
    gradient_w = np.zeros_like(w)
    if (slack_variables > 0):
      gradient_w += -C * len(X) * y * X
    else:
      gradient_w = 0

    gradient_b = 0

    return gradient_w, gradient_b

# SVM.
def train_svm(X, y, C, T, gamma0, a, LR):
    w = np.zeros(X.shape[1])
    b = 0
    objective_values = []

    for t in range(T):
        # Shuffle training data
        np.random.shuffle(X)
        np.random.shuffle(y)

        for i in range(len(X)):
            x_i = X[i]
            y_i = y[i]

            # Calculate gradient
            gradient_w, gradient_b = gradient(w, b, x_i, y_i, C)

            # Apply learning rate schedule
            if (LR):
              gamma_t = gamma0 / (1 + gamma0*t/a )
            else:
              gamma_t = gamma0 / (1 + t)
            # Update model parameters
            w -= gamma_t * gradient_w
            b -= gamma_t * gradient_b

            # Calculate objective value
            objective_values.append(objective_function(w, b, X, y, C))

    return w, b, objective_values

"""## a"""

# Print for different C values of first learning rate.
C_values = [100 / 873, 500 / 873, 700 / 873]
LR = True
for C in C_values:
    w, b, objective_values = train_svm(train_X_scaled, train_y, C, 100, 1, 100, LR)

    # Train error
    train_pred = np.sign(np.dot(train_X_scaled, w) + b)
    train_error = np.mean(train_pred != train_y)
    print(f"C={C}: Train error first learning rate = {train_error:.4f}")

    # Test error
    test_pred = np.sign(np.dot(test_X_scaled, w) + b)
    test_error = np.mean(test_pred != test_y)
    print(f"C={C}: Test error first learning rate = {test_error:.4f}")

"""## b"""

# Print for different C values of first learning rate.
LR = False
for C in C_values:
    w, b, objective_values = train_svm(train_X_scaled, train_y, C, 100, 1, 100, LR)

    # Train error
    train_pred = np.sign(np.dot(train_X_scaled, w) + b)
    train_error = np.mean(train_pred != train_y)
    print(f"C={C}: Train error second learning rate = {train_error:.4f}")

    # Test error
    test_pred = np.sign(np.dot(test_X_scaled, w) + b)
    test_error = np.mean(test_pred != test_y)
    print(f"C={C}: Test error second learning rate = {test_error:.4f}")

"""## c"""



"""# 3

## a
"""



"""## b"""



"""## c"""

