# -*- coding: utf-8 -*-
"""Three Layer Neural Network

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11_S63zmbQW-qg78uFCB-OdIZ3ZCWG7eq
"""

import numpy as np

# Load data
inputs = np.loadtxt("train.csv", delimiter=",")
targets = np.loadtxt("test.csv", delimiter=",")[:, -1]
test_data = np.loadtxt("test.csv", delimiter=",")[:, :-1]

# Define hyperparameters
n_inputs = inputs.shape[1]
n_hidden1 = 3  # Change this value to modify the number of units in hidden layer 1
n_hidden2 = 3  # Change this value to modify the number of units in hidden layer 2
n_outputs = 1
learning_rate = 0.1
epochs = 1


def sigmoid(x):
  """
  Sigmoid activation function.
  """
  return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
  """
  Derivative of the sigmoid function.
  """
  return sigmoid(x) * (1 - sigmoid(x))

def initialize_weights(n_inputs, n_hidden1, n_hidden2, n_outputs):
  """
  Initializes weights randomly.
  """
  weights = {}
  weights['w1'] = np.random.randn(n_inputs, n_hidden1)
  weights['w2'] = np.random.randn(n_hidden1, n_hidden2)
  weights['w3'] = np.random.randn(n_hidden2, n_outputs)
  return weights

def forward_propagation(weights, inputs):
  """
  Performs forward propagation through the network.
  """
  z1 = np.dot(inputs, weights['w1'])
  a1 = sigmoid(z1)
  z2 = np.dot(a1, weights['w2'])
  a2 = sigmoid(z2)
  z3 = np.dot(a2, weights['w3'])
  a3 = sigmoid(z3)
  return a1, a2, a3

def backpropagation(weights, inputs, target, learning_rate):
  """
  Performs backpropagation to update weights.
  """

  # 1. Initialize dL/dw^h_{mn} to zero.
  weights_gradients = {key: np.zeros_like(value) for key, value in weights.items()}

  # Forward propagation
  a1, a2, a3 = forward_propagation(weights, inputs)

  # 2. Find all paths from z^h_n to the output node y
  # (This is implicit in the code, as we know the network structure)

  # 3. For each path s...
  path = [a3]  # Start with the output node
  while len(path) > 1:
    # 3.1. For each node z in s...
    current_node = path.pop()
    previous_node = path[-1]

    # 3.1.1. Compute the partial derivative of z's parent over z.
    # If the node is the output node y, then compute dL/dy
    derivative = sigmoid_derivative(current_node)
    if current_node is a3:
      derivative *= (a3 - target)  # dL/dy

    # 3.1.2. Multiply all the partial derivatives along the path s to obtain g_s.
    g_s = np.prod(derivative, axis=0)

    # 3.2.3. Add to the derivative. dL/d2^h_{mn} becomes dL/d2^h_{mn} + g_s* (dz^h_n)/(dw^h_{mn}).
    for weight_name in weights.keys():
      if weight_name.endswith(f"_{previous_node}"):
        weights_gradients[weight_name] += g_s * (current_node / weights[weight_name]) * previous_node

  # Update weights with gradients
  for key, gradient in weights_gradients.items():
    weights[key] -= learning_rate * gradient

  return weights

def train_network(weights, inputs, targets, learning_rate, epochs):
  """
  Trains the network for a certain number of epochs.
  """
  for epoch in range(epochs):
    for i in range(len(inputs)):
      weights = backpropagation(weights, inputs[i], targets[i], learning_rate)

  return weights

# Initialize weights
# weights = initialize_weights(n_inputs, n_hidden1, n_hidden2, n_outputs)

# Train the network
# weights = train_network(weights, inputs, targets, learning_rate, epochs)

# Make predictions on test data
# predictions = forward_propagation(weights, test_data)[2]

print("Model has been trained")